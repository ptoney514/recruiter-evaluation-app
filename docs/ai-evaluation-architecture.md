# AI Resume Evaluation & Ranking Architecture

## Overview
This document explains how Resume Scanner Pro evaluates individual resumes and ranks them against job descriptions using AI (Claude 3.5 Haiku or GPT-4o Mini).

---

## System Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           USER UPLOADS RESUMES                           â”‚
â”‚                          (Batch: 1-50 PDFs/TXT)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CLIENT-SIDE PDF TEXT EXTRACTION                       â”‚
â”‚                        (frontend/utils/pdfParser)                        â”‚
â”‚  â€¢ PDF.js parses PDF files                                              â”‚
â”‚  â€¢ Extracts plain text from each resume                                 â”‚
â”‚  â€¢ Stores in sessionStorage/Supabase                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     JOB DESCRIPTION + REQUIREMENTS                       â”‚
â”‚  â€¢ Job title, summary, department                                       â”‚
â”‚  â€¢ Must-have requirements (array of strings)                            â”‚
â”‚  â€¢ Preferred requirements (array of strings)                            â”‚
â”‚  â€¢ Additional instructions (optional)                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BATCH EVALUATION ORCHESTRATION                        â”‚
â”‚                   (frontend/services/evaluationService.js)               â”‚
â”‚                                                                          â”‚
â”‚  â€¢ evaluateWithAI(job, candidates[], options)                           â”‚
â”‚  â€¢ Process candidates in parallel (3 concurrent workers)                â”‚
â”‚  â€¢ Each worker calls API for individual candidate                       â”‚
â”‚  â€¢ Retry logic: 2 retries per candidate if API fails                    â”‚
â”‚  â€¢ Progress callback updates UI in real-time                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Parallel (max 3 at once)â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                        â”‚                        â”‚
        â–¼                        â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Candidate 1  â”‚        â”‚  Candidate 2  â”‚       â”‚  Candidate 3  â”‚
â”‚  (individual) â”‚        â”‚  (individual) â”‚       â”‚  (individual) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                        â”‚                        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INDIVIDUAL RESUME EVALUATION                          â”‚
â”‚                       (Python API Serverless)                            â”‚
â”‚                                                                          â”‚
â”‚  POST /api/evaluate_candidate                                           â”‚
â”‚  {                                                                       â”‚
â”‚    job: { title, requirements, summary, ... },                          â”‚
â”‚    candidate: { name, text, email },                                    â”‚
â”‚    stage: 1,                                                             â”‚
â”‚    provider: "anthropic" | "openai",                                    â”‚
â”‚    model: "claude-3-5-haiku-20241022" | "gpt-4o-mini"                   â”‚
â”‚  }                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         AI EVALUATOR MODULE                              â”‚
â”‚                        (api/ai_evaluator.py)                             â”‚
â”‚                                                                          â”‚
â”‚  1. Load recruiting skill instructions (SKILL.md or fallback)           â”‚
â”‚  2. Build Stage 1 evaluation prompt:                                    â”‚
â”‚     â€¢ System context (recruiting expert, two-stage framework)           â”‚
â”‚     â€¢ Job details (title, requirements, summary)                        â”‚
â”‚     â€¢ Candidate resume text                                             â”‚
â”‚     â€¢ Structured output format (scores, recommendation, etc.)           â”‚
â”‚  3. Call LLM provider (Anthropic/OpenAI)                                â”‚
â”‚  4. Parse response using regex                                          â”‚
â”‚  5. Return structured evaluation data                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚                             â”‚
                  â–¼                             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Anthropic Provider  â”‚      â”‚   OpenAI Provider    â”‚
    â”‚   (llm_providers.py) â”‚      â”‚  (llm_providers.py)  â”‚
    â”‚                      â”‚      â”‚                      â”‚
    â”‚  â€¢ Claude 3.5 Haiku  â”‚      â”‚  â€¢ GPT-4o Mini       â”‚
    â”‚  â€¢ $0.003/eval       â”‚      â”‚  â€¢ $0.001/eval       â”‚
    â”‚  â€¢ 30s response time â”‚      â”‚  â€¢ 10s response time â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚                             â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚         LLM API CALL            â”‚
            â”‚  (Claude Messages or OpenAI)    â”‚
            â”‚  â€¢ Max tokens: 4096             â”‚
            â”‚  â€¢ Returns text response        â”‚
            â”‚  â€¢ Tracks token usage           â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       STRUCTURED LLM RESPONSE                            â”‚
â”‚                                                                          â”‚
â”‚  SCORE: 88                                                               â”‚
â”‚  QUALIFICATIONS_SCORE: 92                                               â”‚
â”‚  EXPERIENCE_SCORE: 85                                                   â”‚
â”‚  RISK_FLAGS_SCORE: 88                                                   â”‚
â”‚  RECOMMENDATION: ADVANCE TO INTERVIEW                                   â”‚
â”‚                                                                          â”‚
â”‚  KEY_STRENGTHS:                                                         â”‚
â”‚  - 8+ years Python experience with Flask, Django                        â”‚
â”‚  - Strong ML background (scikit-learn, TensorFlow)                      â”‚
â”‚  - Led team of 5 engineers at previous company                          â”‚
â”‚                                                                          â”‚
â”‚  KEY_CONCERNS:                                                          â”‚
â”‚  - No React experience (job requires full-stack)                        â”‚
â”‚  - Gap in employment (2022-2023)                                        â”‚
â”‚  - Limited startup experience                                           â”‚
â”‚                                                                          â”‚
â”‚  INTERVIEW_QUESTIONS:                                                   â”‚
â”‚  1. Can you walk us through the employment gap in 2022-2023?            â”‚
â”‚  2. How would you approach learning React for this full-stack role?     â”‚
â”‚  3. Tell us about a time you led a team through a technical challenge   â”‚
â”‚                                                                          â”‚
â”‚  REASONING:                                                             â”‚
â”‚  [2-3 paragraphs explaining the score and recommendation]               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        RESPONSE PARSING                                  â”‚
â”‚                      (parse_stage1_response)                             â”‚
â”‚                                                                          â”‚
â”‚  â€¢ Regex extraction of scores (SCORE: 88 â†’ score: 88)                   â”‚
â”‚  â€¢ Parse recommendation (RECOMMENDATION: X â†’ recommendation: X)          â”‚
â”‚  â€¢ Extract bulleted lists (KEY_STRENGTHS, KEY_CONCERNS)                 â”‚
â”‚  â€¢ Extract numbered questions (INTERVIEW_QUESTIONS)                     â”‚
â”‚  â€¢ Extract reasoning paragraphs                                         â”‚
â”‚                                                                          â”‚
â”‚  Returns JavaScript object:                                             â”‚
â”‚  {                                                                       â”‚
â”‚    score: 88,                                                            â”‚
â”‚    qualifications_score: 92,                                            â”‚
â”‚    experience_score: 85,                                                â”‚
â”‚    risk_flags_score: 88,                                                â”‚
â”‚    recommendation: "ADVANCE TO INTERVIEW",                              â”‚
â”‚    key_strengths: ["...", "...", "..."],                                â”‚
â”‚    key_concerns: ["...", "...", "..."],                                 â”‚
â”‚    interview_questions: ["...", "...", "..."],                          â”‚
â”‚    reasoning: "..."                                                     â”‚
â”‚  }                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      API RESPONSE WITH METADATA                          â”‚
â”‚                                                                          â”‚
â”‚  {                                                                       â”‚
â”‚    success: true,                                                        â”‚
â”‚    stage: 1,                                                             â”‚
â”‚    evaluation: { /* parsed evaluation above */ },                       â”‚
â”‚    usage: {                                                              â”‚
â”‚      input_tokens: 1250,                                                 â”‚
â”‚      output_tokens: 420,                                                 â”‚
â”‚      cost: 0.0028  // Calculated based on provider pricing              â”‚
â”‚    },                                                                    â”‚
â”‚    model: "claude-3-5-haiku-20241022",                                  â”‚
â”‚    provider: "anthropic"                                                â”‚
â”‚  }                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BATCH RESULTS AGGREGATION                             â”‚
â”‚                   (evaluationService.js - after all)                     â”‚
â”‚                                                                          â”‚
â”‚  1. Collect all individual evaluation results                           â”‚
â”‚  2. Sort by score (descending): results.sort((a,b) => b.score - a.score)â”‚
â”‚  3. Generate summary statistics:                                        â”‚
â”‚     â€¢ Total candidates                                                  â”‚
â”‚     â€¢ Count by recommendation (Interview/Phone/Decline)                 â”‚
â”‚     â€¢ Top candidate name and score                                      â”‚
â”‚  4. Aggregate usage metrics:                                            â”‚
â”‚     â€¢ Total input tokens (sum across all candidates)                    â”‚
â”‚     â€¢ Total output tokens (sum across all candidates)                   â”‚
â”‚     â€¢ Total cost (sum)                                                  â”‚
â”‚     â€¢ Average cost per candidate                                        â”‚
â”‚                                                                          â”‚
â”‚  Returns:                                                               â”‚
â”‚  {                                                                       â”‚
â”‚    success: true,                                                        â”‚
â”‚    results: [ /* sorted by score desc */ ],                             â”‚
â”‚    summary: {                                                           â”‚
â”‚      totalCandidates: 12,                                               â”‚
â”‚      advanceToInterview: 3,                                             â”‚
â”‚      phoneScreen: 5,                                                    â”‚
â”‚      declined: 4,                                                       â”‚
â”‚      topCandidate: "Sarah Chen",                                        â”‚
â”‚      topScore: 92                                                       â”‚
â”‚    },                                                                    â”‚
â”‚    usage: {                                                             â”‚
â”‚      inputTokens: 15000,                                                â”‚
â”‚      outputTokens: 5040,                                                â”‚
â”‚      cost: 0.034,                                                       â”‚
â”‚      avgCostPerCandidate: 0.0028                                        â”‚
â”‚    }                                                                     â”‚
â”‚  }                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         RESULTS DISPLAY (UI)                             â”‚
â”‚                        (frontend/pages/ResultsPage)                      â”‚
â”‚                                                                          â”‚
â”‚  â€¢ Ranked list of candidates (sorted by score)                          â”‚
â”‚  â€¢ Color-coded by recommendation:                                       â”‚
â”‚    - Green: ADVANCE TO INTERVIEW (85+)                                  â”‚
â”‚    - Yellow: PHONE SCREEN FIRST (70-84)                                 â”‚
â”‚    - Red: DECLINE (<70)                                                 â”‚
â”‚  â€¢ Expandable cards showing:                                            â”‚
â”‚    - Scores breakdown (Qualifications, Experience, Risk Flags)          â”‚
â”‚    - Key strengths and concerns                                         â”‚
â”‚    - Interview questions                                                â”‚
â”‚    - AI reasoning                                                       â”‚
â”‚  â€¢ Summary statistics at top                                            â”‚
â”‚  â€¢ Total cost and token usage                                           â”‚
â”‚  â€¢ Export to PDF option                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Detailed Component Breakdown

### 1. Individual Resume Evaluation Process

#### **Step 1: Prompt Construction** (api/ai_evaluator.py:40-107)

The `build_stage1_prompt()` function creates a structured prompt containing:

**Skill Instructions:**
```
You are evaluating a candidate using a two-stage framework.

Stage 1: Resume Screening (0-100 score)
- Score based on: Qualifications (40%) + Experience (40%) + Risk Flags (20%)
- Thresholds: 85+ = Interview, 70-84 = Phone Screen, <70 = Decline
```

**Job Details:**
- Title, department, location, employment type
- Must-have requirements (bullet list)
- Preferred requirements (bullet list)
- Job summary/description

**Candidate Data:**
- Name and email
- Full resume text (extracted from PDF)

**Output Format Specification:**
```
SCORE: [0-100]
QUALIFICATIONS_SCORE: [0-100]
EXPERIENCE_SCORE: [0-100]
RISK_FLAGS_SCORE: [0-100]
RECOMMENDATION: [ADVANCE TO INTERVIEW / PHONE SCREEN FIRST / DECLINE]

KEY_STRENGTHS:
- [Strength 1]
- [Strength 2]
- [Strength 3]

KEY_CONCERNS:
- [Concern 1]
- [Concern 2]

INTERVIEW_QUESTIONS:
1. [Question 1]
2. [Question 2]
3. [Question 3]

REASONING:
[Explanation paragraphs]
```

#### **Step 2: LLM API Call** (api/llm_providers.py)

**Anthropic Claude 3.5 Haiku:**
```python
client.messages.create(
    model="claude-3-5-haiku-20241022",
    max_tokens=4096,
    messages=[{"role": "user", "content": prompt}]
)

# Pricing: $0.25/1M input, $1.25/1M output
# Average cost: ~$0.003 per evaluation
# Response time: ~20-30 seconds
```

**OpenAI GPT-4o Mini:**
```python
client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {"role": "system", "content": "You are an expert recruiter..."},
        {"role": "user", "content": prompt}
    ],
    max_tokens=4096
)

# Pricing: $0.15/1M input, $0.60/1M output
# Average cost: ~$0.001 per evaluation (3x cheaper!)
# Response time: ~10-15 seconds (2x faster!)
```

#### **Step 3: Response Parsing** (api/ai_evaluator.py:110-184)

Regex-based extraction:
- `SCORE: 88` â†’ `evaluation['score'] = 88`
- `RECOMMENDATION: ADVANCE TO INTERVIEW` â†’ `evaluation['recommendation'] = "ADVANCE TO INTERVIEW"`
- Lines starting with `- ` under `KEY_STRENGTHS:` â†’ added to array
- Lines starting with `1.` under `INTERVIEW_QUESTIONS:` â†’ added to array

Handles malformed responses gracefully:
- Missing scores default to 0
- Missing recommendation defaults to "DECLINE"
- Empty arrays if sections not found

#### **Step 4: Return Structured Data**

```javascript
{
  success: true,
  stage: 1,
  evaluation: {
    score: 88,
    qualifications_score: 92,
    experience_score: 85,
    risk_flags_score: 88,
    recommendation: "ADVANCE TO INTERVIEW",
    key_strengths: [...],
    key_concerns: [...],
    interview_questions: [...],
    reasoning: "..."
  },
  usage: {
    input_tokens: 1250,
    output_tokens: 420,
    cost: 0.0028
  },
  model: "claude-3-5-haiku-20241022",
  provider: "anthropic"
}
```

---

### 2. Batch Evaluation & Ranking Process

#### **Step 1: Parallel Processing** (frontend/services/evaluationService.js:180-236)

```javascript
// Process 3 candidates at a time (concurrency = 3)
// Respects 10/min rate limit for local dev
await processBatch(
  candidates,
  evaluateFn,
  concurrency = 3,
  onProgress
)
```

**Worker Pool Pattern:**
```
Queue: [C1, C2, C3, C4, C5, C6, C7, C8, C9, C10]

Worker 1: C1 â†’ C4 â†’ C7 â†’ C10
Worker 2: C2 â†’ C5 â†’ C8
Worker 3: C3 â†’ C6 â†’ C9

Timeline:
0s:   W1=C1, W2=C2, W3=C3  (3 parallel)
30s:  W1=C4, W2=C5, W3=C6  (next 3)
60s:  W1=C7, W2=C8, W3=C9  (next 3)
90s:  W1=C10               (last one)
```

Each worker:
1. Grabs next candidate from queue
2. Calls `evaluateSingleCandidate()`
3. Updates progress callback
4. Repeats until queue empty

#### **Step 2: Retry Logic** (frontend/services/evaluationService.js:88-169)

```javascript
// If API call fails, retry up to 2 times
async function evaluateSingleCandidate(job, candidate, options, retryCount = 0) {
  try {
    // Call API
  } catch (error) {
    if (retryCount < 2) {
      await sleep(2000)  // Wait 2s
      return evaluateSingleCandidate(job, candidate, options, retryCount + 1)
    }

    // Max retries reached, return error result
    return {
      success: false,
      evaluation: {
        name: candidate.name,
        score: 0,
        recommendation: 'ERROR',
        error: error.message
      }
    }
  }
}
```

#### **Step 3: Results Aggregation** (frontend/services/evaluationService.js:290-340)

After all candidates evaluated:

**1. Collect Results:**
```javascript
const results = evaluationResults.map(r => r.evaluation)
```

**2. Sort by Score (Descending):**
```javascript
results.sort((a, b) => b.score - a.score)
// Now: [Sarah: 92, John: 88, Alice: 85, Bob: 72, ...]
```

**3. Generate Summary Statistics:**
```javascript
const summary = {
  totalCandidates: results.length,
  advanceToInterview: results.filter(r => r.recommendation === 'ADVANCE TO INTERVIEW').length,
  phoneScreen: results.filter(r => r.recommendation === 'PHONE SCREEN FIRST').length,
  declined: results.filter(r => r.recommendation === 'DECLINE').length,
  errors: results.filter(r => r.recommendation === 'ERROR').length,
  topCandidate: results[0].name,  // First after sort
  topScore: results[0].score
}
```

**4. Aggregate Token Usage:**
```javascript
const usage = {
  inputTokens: sum(all input tokens),
  outputTokens: sum(all output tokens),
  cost: sum(all costs),
  avgCostPerCandidate: totalCost / candidateCount
}
```

#### **Step 4: Return Final Rankings**

```javascript
{
  success: true,
  results: [
    { name: "Sarah Chen", score: 92, recommendation: "ADVANCE TO INTERVIEW", ... },
    { name: "John Doe", score: 88, recommendation: "ADVANCE TO INTERVIEW", ... },
    { name: "Alice Wang", score: 85, recommendation: "ADVANCE TO INTERVIEW", ... },
    { name: "Bob Smith", score: 72, recommendation: "PHONE SCREEN FIRST", ... },
    { name: "Jane Lee", score: 68, recommendation: "DECLINE", ... }
  ],
  summary: {
    totalCandidates: 12,
    advanceToInterview: 3,
    phoneScreen: 5,
    declined: 4,
    topCandidate: "Sarah Chen",
    topScore: 92
  },
  usage: {
    inputTokens: 15000,
    outputTokens: 5040,
    cost: 0.034,
    avgCostPerCandidate: 0.0028
  }
}
```

---

## Scoring Formula (Stage 1: Resume Screening)

### Component Weights

```
Total Score = (Qualifications Ã— 0.4) + (Experience Ã— 0.4) + (Risk Flags Ã— 0.2)
```

**Example:**
```
Qualifications: 92/100
Experience: 85/100
Risk Flags: 88/100 (higher = fewer flags)

Total = (92 Ã— 0.4) + (85 Ã— 0.4) + (88 Ã— 0.2)
      = 36.8 + 34.0 + 17.6
      = 88.4/100
```

### Recommendation Thresholds

```
Score â‰¥ 85  â†’ ADVANCE TO INTERVIEW
70 â‰¤ Score < 85 â†’ PHONE SCREEN FIRST
Score < 70  â†’ DECLINE
```

### Scoring Criteria

**Qualifications (40%):**
- Education match (degree level, field)
- Required skills coverage
- Certifications/licenses
- Technical proficiency

**Experience (40%):**
- Years of experience in relevant roles
- Industry experience match
- Progressive responsibility
- Leadership/management experience
- Project complexity and scale

**Risk Flags (20%):**
- Employment gaps
- Job hopping (many short tenures)
- Overqualification (might leave quickly)
- Underqualification (steep learning curve)
- Geographic mismatch
- Salary expectation mismatch
- Cultural fit concerns

---

## Ranking Logic

### Primary Sort: Score (Descending)

```javascript
results.sort((a, b) => b.score - a.score)
```

Candidates ranked purely by numerical score:
```
1. Sarah Chen       92  â­ Interview
2. John Doe         88  â­ Interview
3. Alice Wang       85  â­ Interview
4. Bob Smith        72  ğŸ“ Phone Screen
5. Jane Lee         68  âŒ Decline
```

### No Secondary Sorts

Currently, **ranking is purely score-based**. No tiebreakers.

If two candidates have the same score, order is undefined (based on array order, which depends on API response timing).

**Future Enhancement Idea:**
```javascript
// Tiebreaker: Higher qualifications score
results.sort((a, b) => {
  if (b.score !== a.score) return b.score - a.score
  return b.qualifications_score - a.qualifications_score
})
```

---

## Key Design Decisions

### Why Regex Parsing Instead of JSON?

**Current Approach:**
```
SCORE: 88
QUALIFICATIONS_SCORE: 92
...
```

**Alternative (JSON):**
```json
{
  "score": 88,
  "qualifications_score": 92
}
```

**Reasons for text parsing:**
1. **Natural language prompts are easier to iterate on** - Can add/modify sections without breaking JSON schema
2. **LLMs sometimes break JSON syntax** - Extra commas, unescaped quotes, etc.
3. **Reasoning section benefits from prose** - More readable for humans
4. **Simpler debugging** - Can read raw response easily
5. **Fallback-friendly** - If one field fails to parse, others still work

**Trade-off:** More fragile parsing logic, but more flexible prompt engineering.

### Why Parallel Processing with Concurrency Limit?

**Sequential (slow):**
```
C1 â†’ C2 â†’ C3 â†’ C4 â†’ C5
Total time: 5 Ã— 30s = 150 seconds (2.5 minutes)
```

**Fully Parallel (rate limit violations):**
```
All 50 at once â†’ 10/min rate limit exceeded â†’ failures
```

**Concurrency-Limited Parallel (optimal):**
```
3 at a time â†’ 50 Ã· 3 = 17 batches Ã— 30s = 510 seconds (8.5 minutes)
With retries and errors, ~10 minutes for 50 candidates
```

**Configuration:**
- Local dev: 3 concurrent (10/min rate limit)
- Production: Could increase to 5-10 concurrent with higher limits

### Why Stage 1 Only?

**Stage 2 is planned but not implemented:**
```python
# Stage 2: Final Hiring Decision
# Score = (Resume 25%) + (Interview 50%) + (References 25%)

if stage == 2:
    raise NotImplementedError('Stage 2 evaluation not yet implemented')
```

**Current focus:** Stage 1 (Resume Screening) is the MVP.

**Stage 2 requirements:**
- Interview ratings input (UI not built)
- Reference check data (UI not built)
- Different scoring formula (25/50/25 split)

---

## Performance Metrics

### Evaluation Speed

**Single Candidate:**
- Claude Haiku: 20-30 seconds
- GPT-4o Mini: 10-15 seconds

**Batch of 10 Candidates:**
- Claude: ~100 seconds (3 concurrent workers)
- GPT-4o: ~50 seconds

**Batch of 50 Candidates:**
- Claude: ~500 seconds (8.3 minutes)
- GPT-4o: ~250 seconds (4.2 minutes)

### Cost per Evaluation

**Claude 3.5 Haiku:**
- Input: $0.25/1M tokens
- Output: $1.25/1M tokens
- Avg: ~$0.003 per evaluation

**GPT-4o Mini:**
- Input: $0.15/1M tokens
- Output: $0.60/1M tokens
- Avg: ~$0.001 per evaluation (3x cheaper!)

**Batch of 50:**
- Claude: ~$0.15
- GPT-4o: ~$0.05

### Token Usage

**Average per Evaluation:**
- Input tokens: ~1,200 (job description + resume + prompt)
- Output tokens: ~400 (scores, strengths, concerns, questions, reasoning)
- Total: ~1,600 tokens per candidate

**Batch of 50:**
- Total tokens: ~80,000
- Cost (Claude): ~$0.15
- Cost (GPT-4o): ~$0.05

---

## Error Handling

### API Failures

**Network Errors:**
```javascript
// Retry up to 2 times with 2s delay
if (retryCount < 2) {
  await sleep(2000)
  return evaluateSingleCandidate(..., retryCount + 1)
}

// After max retries, return error result
return {
  success: false,
  evaluation: {
    name: candidate.name,
    score: 0,
    recommendation: 'ERROR',
    error: error.message
  }
}
```

**Timeout Protection:**
```javascript
// 90 second timeout for AI evaluation
fetchWithTimeout(url, options, 90000)

// If timeout, retry or mark as error
```

### Malformed LLM Responses

**Missing Scores:**
```python
# Default to 0 if parsing fails
try:
    evaluation['score'] = int(line.split(':', 1)[1].strip())
except:
    pass  # Defaults to 0 from initialization
```

**Missing Sections:**
```python
# Initialize with empty defaults
evaluation = {
    'score': 0,
    'recommendation': 'DECLINE',
    'key_strengths': [],  # Empty if not found
    'key_concerns': [],
    'interview_questions': [],
    'reasoning': ''
}
```

### Storage Limits

**Client-side storage check:**
```javascript
// Check before adding resume
if (!sessionStore.canAddMoreData(textSizeKB)) {
  errors.push(`Storage limit reached. Try evaluating current batch first.`)
  return null
}
```

---

## Extracting as Standalone Agent

### Core Files Needed

```
standalone-resume-evaluator/
â”œâ”€â”€ ai_evaluator.py          # Core evaluation logic
â”œâ”€â”€ llm_providers.py         # Multi-LLM abstraction
â”œâ”€â”€ requirements.txt         # anthropic, openai
â””â”€â”€ README.md
```

### Standalone Usage

```python
from ai_evaluator import evaluate_candidate_with_ai

job = {
    'title': 'Senior Python Engineer',
    'must_have_requirements': [
        '5+ years Python experience',
        'Flask or Django framework',
        'PostgreSQL/MySQL'
    ],
    'preferred_requirements': [
        'ML/AI experience',
        'Docker/Kubernetes'
    ],
    'summary': 'Build backend services for AI-powered SaaS product'
}

candidate = {
    'full_name': 'Sarah Chen',
    'email': 'sarah@example.com',
    'resume_text': '[Full resume text from PDF extraction]'
}

# Evaluate with Claude
result = evaluate_candidate_with_ai(
    job_data=job,
    candidate_data=candidate,
    stage=1,
    provider='anthropic',
    model='claude-3-5-haiku-20241022'
)

print(f"Score: {result['evaluation']['score']}/100")
print(f"Recommendation: {result['evaluation']['recommendation']}")
print(f"Cost: ${result['usage']['cost']}")
print(f"Key Strengths: {result['evaluation']['key_strengths']}")
```

### Integration Points

**As Python Function:**
```python
# Import and call directly
from ai_evaluator import evaluate_candidate_with_ai
```

**As REST API:**
```python
# Flask/FastAPI wrapper
@app.post('/evaluate')
def evaluate(job: Job, candidate: Candidate):
    return evaluate_candidate_with_ai(job, candidate, stage=1)
```

**As CLI Tool:**
```bash
python evaluate.py \
  --job job.json \
  --resume resume.txt \
  --provider anthropic \
  --model claude-3-5-haiku-20241022
```

**As Lambda Function:**
```python
# Serverless deployment
def lambda_handler(event, context):
    job = event['job']
    candidate = event['candidate']
    return evaluate_candidate_with_ai(job, candidate, stage=1)
```

---

## Summary

**Individual Evaluation:**
1. Resume text extracted (PDF â†’ plain text)
2. Prompt built (job + candidate + output format)
3. LLM called (Claude or GPT-4o)
4. Response parsed (regex extraction)
5. Structured data returned (score, recommendation, insights)

**Batch Ranking:**
1. Process candidates in parallel (3 workers)
2. Retry logic handles failures
3. Results collected and sorted by score
4. Summary statistics generated
5. Total cost and usage aggregated

**Key Features:**
- Multi-LLM support (Claude Haiku, GPT-4o Mini)
- Structured scoring (Qualifications 40% + Experience 40% + Risk 20%)
- Automatic ranking (sort by score descending)
- Cost optimization (GPT-4o Mini 3x cheaper than Claude)
- Error resilience (retries, timeouts, graceful degradation)
- Progress tracking (real-time UI updates)

**Standalone Extraction:**
- Core logic in `ai_evaluator.py` + `llm_providers.py`
- No frontend dependencies
- Can be used as Python library, API, CLI, or serverless function
- Requires: `anthropic` and/or `openai` SDK, Python 3.13+
